{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bf6d03-4480-48dc-9f3e-5fd28ac792ee",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "#### Reinforcement Learning is a common Machine Learning techniques where the model learns through trail and error. Reinforcement Learning is commonly used for video games, and robotics. The common features of RL are listed below:\n",
    "- Agent: Learner who decides where to move\n",
    "- Policy: A rule that guides the decision process of the agent\n",
    "- Environment: The area in where all possible states are\n",
    "- State: Current state of agent\n",
    "- Action: The act of moving from one state to another\n",
    "- Goal State: the state which agent is looking for. This is the state with reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75960c86-3ff2-4510-9e4c-2df9a5a0abc4",
   "metadata": {},
   "source": [
    "## Robot Game\n",
    "#### In this game we will focus on a robot who will try to make all rooms(states); A, B, C, D clean. The environemnt consists of these four rooms, and will be reffered to as states. Using the policy_evaluation function, we will find the state values, s1, s2, s3, s4. Rewards of each actios are listed below:\n",
    "- Each movement between the rooms has a reward of −0.04.\n",
    "- Suck action has a reward of −0.1.\n",
    "- Reaching the terminal state s5 will lead to a reward of 1.0.\n",
    "- The discount factor is γ = 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5548c0f8-1a94-4c41-8935-a09eb7b5ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The robot aims to clean the dusts in room D. The layout of the rooms are shown below.\n",
    "# --------------\n",
    "# |  A  |  B   |\n",
    "# --------------\n",
    "# |  C  | .D.  |\n",
    "# --------------\n",
    "\n",
    "# Define states\n",
    "states = [\n",
    "    's1',  # at A, D uncleaned\n",
    "    's2',  # at B, D uncleaned\n",
    "    's3',  # at C, D uncleaned\n",
    "    's4',  # at D, D uncleaned\n",
    "    's5'   # at D, D cleaned\n",
    "]\n",
    "\n",
    "# Terminal state is s5, where D is cleaned\n",
    "terminal_states = ['s5']\n",
    "\n",
    "# Define actions\n",
    "actions = ['left', 'right', 'up', 'down', 'suck']\n",
    "\n",
    "# Applicable actions for each state, except terminal states\n",
    "applicable_actions = [\n",
    "    ['right', 'down', 'suck'],  # for state s1\n",
    "    ['left', 'down', 'suck'],  # for state s2\n",
    "    ['right', 'up', 'suck'],  # for state s3\n",
    "    ['left', 'up', 'suck']  # for state s4\n",
    "]\n",
    "\n",
    "# Next state when applying each applicable action in each state\n",
    "next_states = [\n",
    "    ['s2', 's3', 's1'],   # s'(s1, a)\n",
    "    ['s1', 's4', 's2'],   # s'(s2, a)\n",
    "    ['s4', 's1', 's3'],   # s'(s3, a)\n",
    "    ['s3', 's2', 's5']    # s'(s4, a)\n",
    "]\n",
    "\n",
    "# Reward when applying each applicable action in each state\n",
    "rewards = [\n",
    "    [-0.04, -0.04, -0.1],   # r(s1, a)\n",
    "    [-0.04, -0.04, -0.1],   # r(s2, a)\n",
    "    [-0.04, -0.04, -0.1],   # r(s3, a)\n",
    "    [-0.04, -0.04, -0.1]    # r(s4, a)\n",
    "]\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# When reaching the terminal state s5, we receive a reward of 1.0.\n",
    "terminal_reward = 1.0\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c223c950-8bf8-419d-8a14-2e5a59258c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9399999999999995, -0.9999999999999994, -0.8859999999999996, -0.9399999999999995]\n"
     ]
    }
   ],
   "source": [
    "# Q1.1: policy evaluation\n",
    "\n",
    "def policy_evaluation(policy):\n",
    "    V = {}\n",
    "    for s in states:\n",
    "        if s in terminal_states:\n",
    "            V[s] = terminal_reward\n",
    "        else:\n",
    "            V[s] = 0.0\n",
    "        \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i, s in enumerate(states[:-1]):\n",
    "            vold = V[s]\n",
    "            action = policy[s]\n",
    "            a_idx = applicable_actions[i].index(action)\n",
    "            next_state = next_states[i][a_idx]\n",
    "            reward = rewards[i][a_idx]\n",
    "\n",
    "            V[s] = reward + gamma * V[next_state]\n",
    "\n",
    "            delta = max(delta, abs(vold - V[s]))\n",
    "\n",
    "        if delta == 0:\n",
    "            return [V[s] for s in states[:-1]]\n",
    "    \n",
    "\n",
    "\n",
    "pi = {\n",
    "        's1': 'right',\n",
    "        's2': 'suck',\n",
    "        's3': 'up',\n",
    "        's4': 'up'}\n",
    "\n",
    "state_values = policy_evaluation(pi)\n",
    "print(state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be5d6c-5267-4aaf-a906-51a2320ef67b",
   "metadata": {},
   "source": [
    "## Results\n",
    "#### We recieved the state values, [s1= -0.93, s2= -1, s3= -0.89, s4= -0.93]. These values are highly negative. Which shows us that starting in any of these states following the policy will give us bad outcomes. The robot should not follow this policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "838072d9-13fb-4262-9519-27d852038079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5720000000000001, 0.68, 0.68, 0.8]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Q1.2: value iteration\n",
    "###\n",
    "\n",
    "# @TODO\n",
    "# Please complete this value iteration method,\n",
    "# which returns the optimal state values for all the states,\n",
    "# except the terminal states.\n",
    "# Please delete the \"return 0;\" as it is a placeholder.\n",
    "def value_iteration():\n",
    "    V = {}\n",
    "    for s in states:\n",
    "        if s in terminal_states:\n",
    "            V[s] = terminal_reward\n",
    "        else:\n",
    "            V[s] = 0.0\n",
    "        \n",
    "    while True:\n",
    "        delta = 0\n",
    "        qvalue = {}\n",
    "        for i, s in enumerate(states[:-1]):\n",
    "            vold = V[s]\n",
    "            V[s] = 0.8 * (rewards[i][a_idx] + gamma0* V[next_states[i][a_idx]] \n",
    "                       for a_idx in range(len(applicable_actions[i])))\n",
    "                \n",
    "                \n",
    "\n",
    "            delta = max(delta, abs(vold - V[s]))\n",
    "\n",
    "        if delta == 0:\n",
    "            return [V[s] for s in states[:-1]]\n",
    "\n",
    "\n",
    "# Run value iteration\n",
    "state_values = value_iteration()\n",
    "print(state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e6645d-7917-46e4-8480-25639b9d3d2a",
   "metadata": {},
   "source": [
    "## Results\n",
    "- The state values we recived [s1= 0.58, s2= 0.68, s3= 0.68, s4= 0.8], these values differ from previous policy. This is because in this policy, we iterate to find the optimal soluton for each state. The highest value we recieved was from s4, which was a state value of 0.8. This is a high positive value and shows us that this state benefits highly from its action. This is expected because the robot is trying to clean the dirt which is only found in s4. We recieved slightly lower values in state 2 and state 3, because it would take one step before reaching the goal state which is state 4. The lowest state value is recieved from state 1, becuase it would require two steps before reaching goal state, requiring extra costs. \n",
    "- This policy(value iteration) follows a deterministic structure and found the optimal state value for these states by updating the costs until there are no changes. This model considered all possible moves at each state and returned state 4 with highest reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79e681-4e9c-47fb-bdbc-5ddfc33dadd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
